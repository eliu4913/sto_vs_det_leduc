This project investigates the performance, robustness, and exploitability of reinforcement learning agents in the hidden-information game of Leduc Hold’em Poker. We compare three agents—DQN, PPO, and DQN with Boltzmann sampling — using head-to-head matchups and exploitability analysis against a CFR (Counterfactual Regret Minimization) agent. Our results highlight the strengths of stochastic policies, with PPO achieving the best overall performance due to its adaptability and robustness. The DQN Boltzmann agent demonstrated improved performance and reduced exploitability at moderate temperatures, showcasing the benefits of controlled stochasticity. Conversely, the deterministic DQN agent was more predictable and highly exploitable. We also examined exploitability trends, finding a trade-off between determinism and randomness in strategy design. These findings emphasize the importance of balancing exploration and exploitation in reinforcement learning and provide a framework for evaluating agent robustness in hidden-information games. Future work could explore generalizing these methods to more complex environments and implementing adaptive mechanisms for enhanced performance.